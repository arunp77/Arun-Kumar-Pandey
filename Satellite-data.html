<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Data processing</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/Favicon-1.png" rel="icon">
  <link href="assets/img/Favicon-1.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Creating a python code section-->
  <link rel="stylesheet" href="assets/css/prism.css">
  <script src="assets/js/prism.js"></script>


  <!-- To set the icon, visit https://fontawesome.com/account-->
  <script src="https://kit.fontawesome.com/5d25c1efd3.js" crossorigin="anonymous"></script>
  <!-- end of icon-->

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha384-oGqqFhf3ELCpQk69FVb6jGrwPOTR5SO5FeECBbCFgrFJzVpXJFLHc06dL/iPzCBJe" crossorigin="anonymous">

  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/myphoto.jpeg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Arun</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/arunp77/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/arunp77" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/arunp77_" class="twitter"><i class="bx bxl-twitter"></i></a>
          <a href="https://www.instagram.com/arunp77/" class="instagram"><i class="bx bxl-instagram"></i></a>
          <a href="https://medium.com/@arunp77" class="medium"><i class="bx bxl-medium"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#skills-and-tools" class="nav-link scrollto"><i class="bx bx-wrench"></i> <span>Skills and Tools</span></a></li>
          <li><a href="index.html#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Portfolio</span></a></li>
          <li><a href="index.html#services" class="nav-link scrollto"><i class="bx bx-server"></i> <span>Services</span></a></li>
          <li><a href="index.html#professionalcourses" class="nav-link scrollto"><i class="bx bx-book-alt"></i> <span>Professional Certification</span></a></li>
          <li><a href="index.html#publications" class="nav-link scrollto"><i class="bx bx-news"></i> <span>Publications</span></a></li>
          <li><a href="index.html#extra-curricular" class="nav-link scrollto"><i class="bx bx-rocket"></i> <span>Extra-Curricular Activities</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Data processing</h2>
          <ol>
            <li><a href="Remote-sensing-content.html" class="clickable-box"><i class="fas fa-arrow-left"></i> Remote sensing content </i></a></li>
            <li><a href="index.html#portfolio" class="clickable-box"> Go to portfolio <i class="fas fa-arrow-right"></i></a></li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!------  right dropdown menue ------->
    <div class="right-side-list">
        <div class="dropdown">
            <button class="dropbtn"><strong>Shorcuts:</strong></button>
            <div class="dropdown-content">
                <ul>
                    <li><a href="cloud-compute.html"><i class="fas fa-cloud"></i> Cloud</a></li>
                    <li><a href="AWS-GCP.html"><i class="fas fa-cloud"></i> AWS-GCP</a></li>
                    <li><a href="amazon-s3.html"><i class="fas fa-cloud"></i> AWS S3</a></li>
                    <li><a href="ec2-confi.html"><i class="fas fa-server"></i> EC2</a></li>
                    <li><a href="Docker-Container.html"><i class="fab fa-docker" style="color: rgb(19, 18, 18);"></i> Docker</a></li>
                    <li><a href="Jupyter-nifi.html"><i class="fab fa-python" style="color: rgb(17, 16, 16);"></i> Jupyter-nifi</a></li>
                    <li><a href="snowflake-task-stream.html"><i class="fas fa-snowflake"></i> Snowflake</a></li>
                    <li><a href="data-model.html"><i class="fas fa-database"></i> Data modeling</a></li>
                    <li><a href="sql-basics.html"><i class="fas fa-table"></i> QL</a></li>
                    <li><a href="sql-basic-details.html"><i class="fas fa-database"></i> SQL</a></li>
                    <li><a href="Bigquerry-sql.html"><i class="fas fa-database"></i> Bigquerry</a></li>
                    <li><a href="scd.html"><i class="fas fa-archive"></i> SCD</a></li>
                    <li><a href="sql-project.html"><i class="fas fa-database"></i> SQL project</a></li>
                      <!-- Add more subsections as needed -->
                  </ul>
            </div>
          </div>
      </div>


    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">
          <div class="row gy-4">
            <div style="background-color: rgb(115, 228, 243); color: rgb(29, 27, 27); padding: 10px;">
              <h1>Process levels</h1>
          </div>
          <div class="image">
            <figure style="text-align: center;">
                <img src="assets/img/remote-sensing/sentinal=EUMESAT.png" alt="" style="max-width: 80%; max-height: 80%;">
                <br><figcaption style="text-align: center;"><strong>Image credit:</strong><a href="https://www.eumetsat.int/" target="_blank"> EUMESAT</a></figcaption>
            </figure>            
          </div>

          <section id="section-1">
            <h2>Various aspects of Satellite data collection </h2>
            The process of remote sensing, from data collection to final image processing, involves several stages. Here's a detailed description of each stage:
            
            <ol>
              <li><strong>Platform and Sensor Selection: </strong>First step is to choose appropriate satellite, aircraft, or unmanned aerial Vehicle (UAV)
              platform and hence the sensors onboard on the specific objectives of the remote sensing mission. These sensors then captures data in relevat 
              spectral bands for the desired applications (example optical, infrared, radar).</li>
              <li><strong>Mission Planning and Scheduling: </strong>Next step would be to choose the orbits to cover the target area after 
                considering the lighting conditions, weather and other mission constrains.</li>
              <li><strong>Data Acquisition: </strong>Then the sensors onboards acquire remote sensing data through capturning the electromagnetic radiation in various wavelengths. 
                When utilizing optical sensors, the collection of data is accomplished by depending on sunlight reflection and absorption. In contrast, with radar sensors, 
                the sensors emit microwave signals, and the captured data comes from the signals that are backscattered. </li>
              <li><strong>Telemetry and Data Transmission: </strong>In the next step, the acquired data is transmitted to ground stations using telemetry systems. 
                Telemetry data includes details such as the satellite's health, its orientation in space, power levels, and sensor status.</li>
              <li><strong> Data Preprocessing: </strong>In this step, various data corrections (radiometric and geometric distortions introduced during data acquisition) are done.
              Atmospheric correction are also done in this step. The normalization and standardization of pixel values are also done for consistency.</li>
              <li><strong>Image Registration:</strong>Next multiple images from different times or sensors are aligned to a common coordiante system. 
                Also geometric distortions are corrected to enable accurate comparison and analysis.</li>
              <li><strong>Image Enhancement: </strong>Image emhancement is done to improve the visual quality and highlight specific features using techniques like contrast adjustment
              and histogram equalization. If required, specific spectral bands are enhanced to emphaseze certain infromation.</li>
              <li><strong>Image Classification: </strong>Pixels or regions are assigned within the images to predefined classes or land cover types. These are done by using classification 
              algorithms based on spectral, textural and contextual features. </li>
              <li><strong>Feature Extraction:</strong>Finally relevant features are identified and extracted from the images, such as vegitation indices, water bodeis, or urban areas.
              Texture analysis and spatially filtering are used for this purpose. </li>
              <li><strong>Change Detection: </strong>After going through all the above steps, multiple images over times are compared to idnetify changes in land cover or other features. 
              The algorithm is then trained using a training set which includes images from different geographical locations with similar characteristics. </li>
            </ol>

            <h4>Image enhancement process</h4>
            <ol>
              <li><strong>Data Preparation:</strong>
                <ul>
                  <li>Acquire the satellite imagery and corresponding labels. The labels indicate the types of land cover (e.g., forest, water, urban, etc.) present in the images.</li>
                  <li>Reshape the images to have a consistent size and channel ordering. This can be achieved by using Python libraries like NumPy or OpenCV.</li>
                  <li>Normalize the pixel values of the images to ensure consistent scale. This can be achieved by dividing the pixel values by 255.</li>
                  <li>Split the data into training, validation, and testing sets.</li>
                </ul>
              </li>
              <li><strong>Feature Extraction: </strong>
                <ul>
                  <li>Convert the satellite images from their RGB color space to another color space, such as HSV or YCrCb. This can help capture additional information about the land cover 
                    present in the images.</li>
                  <li>Apply a transformation, such as wavelet transformation, to the satellite images. This can help enhance the quality of the images by reducing noise and preserving the 
                    original features of the images.</li>
                </ul>
              </li>
              <li><strong>Model Training:</strong>
                <ul>
                  <li>Select a suitable machine learning model, such as a convolutional neural network (CNN), for the classification task.</li>
                  <li>Train the model on the training set, using the extracted features as inputs and the corresponding labels as targets.</li>
                  <li>Evaluate the performance of the model on the validation set, using appropriate metrics like accuracy, precision, recall, and F1-score.</li>
                </ul>
              </li>
              <li><strong>Model Optimization:</strong>
                <ul>
                  <li>If the model's performance on the validation set is not satisfactory, try to improve the model's architecture or adjust its hyperparameters.</li>
                  <li>Continue training and evaluating the model until its performance is satisfactory. </li>
                </ul>
              </li>
              <li><strong>Model Deployment:</strong>
                <ul>
                  <li>Once the model has been trained and optimized, deploy it to a production environment.</li>
                  <li>Use the deployed model to enhance satellite imagery by classifying the types of land cover present in the images.</li>
                </ul>
              </li>
            </ol>
            <p>By following these steps, you can successfully enhance satellite imagery using machine learning techniques. This can have a 
              significant impact on various applications, such as land use planning, natural resource management, and disaster response.</p>
          
        
          
          <h3>Satellite data collection levels</h3>

          <p>Within remote sensing and its applications, there are a series of levels that are used to define the amount of processing that has been performed to provide a given dataset. 
            Satellite data is collected at various levels, each representing a different stage of processing and refinement. Here are the common levels of satellite data:</p>
          <ol>
            <li><strong>Level 0 (L0):</strong> This is raw data directly from the satellite's sensors and refers to full resolution data. It includes unprocessed digital counts or voltage 
              measurements. L0 data is transmitted to ground stations. It is unlikely you will work with this level of data, especially for more modern sensors, as this data lacks information 
              such as geo-referencing and time-referencing ancillary information.</li>

            <li><strong>Level 1 (L1):</strong> L1 data is generated by applying basic corrections to L0 data. It includes georeferencing, radiometric calibration, and removal of sensor-specific 
              artifacts. The output is still in sensor units. This level also includes quality and classification flags. <strong>Example:</strong> for ocean color, this would be often referred to 
              as the “top of atmosphere” radiance [mW.m-2.sr-1.nm-1].</li>

            <li><strong>Level 2 (L2):</strong> At this level, L1 data is further processed to convert sensor units into physical units (e.g., radiance or reflectance). Atmospheric corrections may 
              also be applied to account for the influence of the atmosphere.</li>

            <li><strong>Level 3 (L3):</strong> L3 data involves data products that are geographically gridded or mapped. This level often includes time-averaged or composited data to create datasets 
              suitable for broader scientific analysis.</li>

            <li><strong>Level 4 (L4):</strong> L4 data represents higher-level data products derived from multiple satellites or sources. It can include global climate datasets, reanalysis data, or 
              models that assimilate satellite observations.</li>
          </ol>

          <figure style="text-align: center;">
            <img src="assets/img/remote-sensing/Level-0-4-stages.png" alt="" style="max-width: 90%; max-height: 90%;">
            <figcaption style="text-align: center;">Level-0 to Level-1 remote sensing data transformation. (<strong>Image credit: © </strong><a href="https://arunp77.github.io/Arun-Kumar-Pandey/" target="_blank"> Arun Kumar Pandey</a>)</figcaption>
          </figure>

          <h3>Data Processing</h3>
          <p>Processing of satellite data typically involves these steps:</p>
          
          <ul>
            <li><strong>Data Reception:</strong> Raw data is received from the satellite by ground stations or satellite receivers.</li>
            <li><strong>Preprocessing:</strong> This step involves initial data quality checks, formatting, and metadata extraction.</li>
            <li><strong>Calibration:</strong> Radiometric and geometric corrections are applied to make the data consistent and accurate.</li>
            <li><strong>Georeferencing:</strong> The data is geolocated to specific geographic coordinates using satellite orbit and attitude information.</li>
            <li><strong>Atmospheric Correction:</strong> Corrections are made to account for the effects of the atmosphere on the data, especially for optical and infrared sensors.</li>
            <li><strong>Data Fusion:</strong> In some cases, data from multiple sensors or satellites are combined to create composite datasets.</li>
            <li><strong>Validation:</strong> The processed data is compared with ground-based measurements or models to ensure its accuracy.</li>
            <li><strong>Data Distribution:</strong> Processed data products are made available to researchers and the public through various data centers and platforms.</li>
          </ul>

          <figure style="text-align: center;">
            <img src="assets/img/remote-sensing/Level-0-4.png" alt="" style="max-width: 90%; max-height: 90%;">
            <figcaption style="text-align: center;"><strong>Image credit:</strong><a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-36699-9_36">Processing Levels, 
              Ron Weaver </a></figcaption>
          </figure>

        </section>

          <section id="Data-Transformation-required">
          <h3 id="Data-Transformation-required">Data Transformation required<a class="anchor-link" href="#Data-Transformation-required">&#182;</a></h3><p>Level 0 (L0) data represents 
            the raw, unprocessed data directly received from a satellite's sensors. Transforming Level 0 data into Level 1 (L1) involves several essential steps to convert the raw 
            sensor measurements into physically meaningful units. Here's a step-by-step process for this transformation:</p>
            <p><strong>Step 1: Data Reception</strong></p>
            <ul>
            <li>The satellite collects raw data from its sensors while orbiting Earth.</li>
            <li>Satellite sensors detect electromagnetic radiation (e.g., visible light, infrared) from the Earth's surface and atmosphere. These sensors produce raw measurements, 
              which are often in the form of digital counts or voltage values. These raw measurements are influenced by various factors, including sensor characteristics, electronics, 
              and atmospheric conditions.</li>
            <li>This raw data is transmitted to ground stations or satellite receivers. The data is typically received in digital form, representing measurements as digital counts or 
              voltage values.</li>
            </ul>
            <p><strong>Step 2: Data Preprocessing</strong></p>
            <ul>
            <li>Initial data quality checks are performed to ensure that the received data is complete and not corrupted during transmission.</li>
            <li>The data may be formatted to meet specific standards and file formats for processing.</li>
            </ul>
            <p><strong>Step 3: Georeferencing</strong></p>
            <ul>
            <li>The satellite's position and orientation information (orbit and attitude data) are used to georeference the raw data. This step involves determining the precise 
              geographic coordinates for each data point.</li>
            <li>Georeferencing is critical for associating the raw measurements with specific locations on Earth's surface.</li>
            </ul>
            <p><strong>Step 4: Radiometric Calibration</strong></p>
            <ul>
            <li>Radiometric calibration is applied to convert the raw sensor counts or voltage measurements into physical units, such as <span style="color:red">radiance or reflectance.</span>.</li>
            <li>Calibration involves characterizing the sensor's response to incoming radiation and correcting for sensor-specific artifacts or biases.</li>
            <li>Calibration coefficients are applied to each data point to ensure consistency and accuracy.</li>
            </ul>
            <blockquote><p>Radiometric calibration is a critical process in remote sensing and satellite imagery that involves correcting and standardizing the raw sensor measurements 
              to ensure that they accurately represent physical 
            properties of the observed scene, such as radiance or reflectance. This calibration is essential to make satellite data consistent, reliable, and suitable for scientific 
            analysis and comparison.</p>
            <ul>
            <li>To perform radiometric calibration, satellite missions often use calibration targets or references with known reflective or emissive properties. These targets may 
              include special panels or surfaces with precisely measured reflectance or radiance values. Alternatively, celestial targets, such as the sun or moon, can be used for 
              calibration when they are in the field of view.</li>
            <li>Radiometric calibration involves characterizing the sensor's response to incoming radiation. This characterization includes understanding how sensor measurements are 
              influenced by factors like sensor gain, offset, and linearity. Sensor-specific characteristics are determined through laboratory tests and measurements.</li>
            </ul>
            </blockquote>
            <p><strong>Step 5: Removal of Sensor Artifacts</strong></p>
            <ul>
            <li>Sensor-specific artifacts, such as detector noise or electronic glitches, are identified and removed from the data.</li>
            <li>This step helps improve data quality by eliminating unwanted variations that may be present in the raw measurements.</li>
            </ul>
            <blockquote><ul>
            <li>Correction algorithms are developed based on the sensor's characteristics and the known properties of calibration targets. These algorithms are used to convert raw 
              sensor measurements into physically meaningful units, such as radiance (for optical sensors) or brightness temperature (for thermal sensors). The algorithms account 
              for sensor-specific biases and errors.</li>
            <li>Radiometric calibration often includes adjustments for sensor gain (amplification) and offset (baseline) to ensure that the measurements accurately represent the 
              radiative properties of the observed scene. These adjustments are made to minimize systematic errors.</li>
            </ul>
            </blockquote>
            <p><strong>Step 6: Atmospheric Correction:</strong></p>
            <ul>
            <li>In addition to sensor-related calibration, radiometric calibration may involve atmospheric correction. This step accounts for the influence of the Earth's atmosphere 
              on incoming radiation. It corrects for atmospheric scattering, absorption, and other effects to obtain surface reflectance or radiance values.</li>
            </ul>
            <p><strong>Step 7: Data Quality Assurance</strong></p>
            <ul>
            <li>Quality control procedures are applied to identify and flag any data points that may still contain errors or anomalies.</li>
            <li>Data quality metrics are generated to assess the overall quality of the L1 data.</li>
            </ul>
            <p><strong>Step 8: Metadata Generation</strong></p>
            <ul>
            <li>Metadata, including information about the satellite, sensor characteristics, calibration parameters, and processing history, is generated and associated with the 
              L1 data. Metadata is crucial for data users to understand the data's context and processing steps.</li>
            </ul>
            <p><strong>Step 9: Output in Physically Meaningful Units</strong></p>
            <ul>
            <li>The L1 data is now in physically meaningful units, such as radiance for optical sensors or brightness temperature for microwave sensors.</li>
            <li>The data is ready for scientific analysis, and researchers can use it for various applications, including environmental monitoring, climate studies, and more.</li>
            </ul>
            <p>The transformation from Level 0 to Level 1 data ensures that the data is accurate, calibrated, and georeferenced, making it suitable for a wide range of scientific 
              and operational purposes. This processed data can be further refined and used to derive higher-level data products at subsequent processing levels (e.g., Level 2, Level 3) 
              for specific scientific applications.</p>
          </section>

            <section id="Voltage-values-in-Level-0-data">
            <h3>Voltage values in Level-0 data</h3>
              <p>In the context of collecting raw data from satellite sensors, "voltage values" refer to one of the ways that certain types of sensors record measurements. 
                When a sensor detects a physical quantity, such as light intensity or temperature, it often generates an electrical signal proportional to that quantity. 
                This electrical signal is typically in the form of voltage.</p>
              <p>Here's a more detailed explanation:</p>
              <ol>
                <li><p><strong>Sensor Output:</strong> Satellite sensors are designed to capture various types of data, such as imagery, temperature, or radiation levels. 
                  When these sensors interact with the environment, they produce an electrical signal that corresponds to the measured quantity.</p>
                </li>
                <li><p><strong>Voltage as a Signal:</strong> In many cases, this electrical signal is in the form of voltage. The magnitude of the voltage signal is directly 
                  related to the quantity being measured. For example, in an optical sensor, the amount of light detected can be translated into a voltage signal where higher 
                  light intensity corresponds to higher voltage values.</p>
                </li>
                <li><p><strong>Analog-to-Digital Conversion:</strong> Before transmitting the data, the analog voltage signal is often converted into a digital representation. 
                  This process is called analog-to-digital conversion (ADC). In ADC, the continuous analog voltage is sampled at discrete intervals and assigned digital values 
                  (digital counts).</p>
                </li>
                <li><p><strong>Digital Counts:</strong> These digital values, referred to as "digital counts" or simply "counts," are then transmitted as part of the raw data. 
                  Each count represents a specific voltage level recorded by the sensor during its measurement.</p>
                </li>
              </ol>
              <p>For instance, if you have an optical sensor on a satellite that measures sunlight intensity, it might generate voltage values as it detects varying levels 
                of sunlight. These voltage values are then digitized and transmitted as part of the raw data to ground stations or receivers for further processing and analysis.</p>
              <p>In summary, "voltage values" in this context refer to the electrical signals generated by satellite sensors to represent the physical quantities they are 
                designed to measure. These signals are converted into digital counts for transmission and subsequent data processing.</p>

                <h3>Reflectance</h3>

                <figure style="text-align: center;">
                  <img src="assets/img/portfolio/spectrum-1.png" alt="" style="max-width: 90%; max-height: 90%;">
                </figure>

              Reflectance in remote sensing is calculated by measuring the amount of electromagnetic radiation (light) that is reflected from a surface. The reflectance values are 
              typically expressed as a percentage or a unitless fraction. The formula for calculating reflectance is as follows:
              $$\text{Reflectance} = \frac{\text{Reflected Radiance}}{\text{Incident Radiance}} \times 100\%$$

              where:
              <ul>
                <li><strong>Reflectance: </strong>The percentage of light that is reflected by a surface. It indicates how much of the incident light is returned.</li>
                <li><strong>Reflected Radiance: </strong>The amount of electromagnetic radiation (light) that is reflected by the surface and measured by the sensor. It is often 
                  represented in radiance units. </li>
                <li><strong>Incident Radiance: </strong>The amount of electromagnetic radiation (light) that reaches the surface. It is the incoming light before interacting with the surface.</li>
              </ul>
              <p>Reflectance values range from 0% (no reflection) to 100% (complete reflection). A reflectance of 0% indicates that the surface absorbs all incident light, while a 
                reflectance of 100% means that the surface reflects all incident light.</p>
              <p>In practice, the reflectance values are often corrected for atmospheric effects and sensor characteristics. The reflectance calculation is an essential step in 
                converting raw remote sensing data into meaningful information that can be used for various applications, such as land cover classification, vegetation health 
                assessment, and environmental monitoring.</p>
              <p>The accuracy of reflectance calculations depends on factors such as sensor calibration, atmospheric correction, and the spectral characteristics of the surface 
                being observed. Remote sensing instruments equipped with different spectral bands capture the reflected radiance at specific wavelengths, allowing scientists to 
                analyze and interpret the Earth's surface properties.</p>

              <h3><strong>Spectral Indices in Remote Sensing</strong></h3>
              <p>Spectral indices in remote sensing are mathematical combinations of reflectance values from different spectral bands. These indices help highlight specific 
                features or characteristics of the Earth's surface, such as vegetation health, water content, or soil moisture.
                <figure style="text-align: center;">
                  <img src="assets/img/remote-sensing/spectral-indexes-1.png" alt="" style="max-width: 90%; max-height: 90%;">
                  <figcaption style="text-align: center;"><strong>Image credit: </strong> Spectral indices mini cubes computed from Sentinel-2 displaying a 2.56 km radius around the DE-Hai
                    site (<a href="https://www.nature.com/articles/s41597-023-02096-0" target="_blank"> Reference paper by David Montero et. al.</a>)</figcaption>
                </figure>

                Here are a few commonly used spectral indices:</p>
              <ol>
                <li><strong>Normalized Difference Vegetation Index (NDVI):</strong><br>
                  (For reference, please see the link <a href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/normalized-difference-vegetation-index" target="_blank">Normalized Difference Vegetation Index</a>)
                  <figure style="text-align: center;">
                    <img src="assets/img/remote-sensing/NDVI.png" alt="" style="max-width: 90%; max-height: 90%;">
                  </figure>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NDVI} = \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+\text{Red})}$$
                    <li><strong>Purpose: </strong> NDVI is widely used to assess vegetation health and density. Healthy vegetation reflects more near-infrared (NIR) light and 
                      absorbs more red light.s</li>
                    <li>NIR (Near-Infrared): Reflectance in the near-infrared region of the electromagnetic spectrum.</li>
                    <li>Red: Reflectance in the red region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Enhanced Vegetation Index (EVI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{EVI} = G\times \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+C_1 \times \text{Red}-C_2 \times \text{Blue}+L)}$$
                    </li>
                    <li><strong>Purpose: </strong>EVI is an enhanced version of NDVI, designed to minimize atmospheric influences and improve sensitivity to vegetation.</li>
                    <li>G (Gain Factor): A gain factor to optimize the sensitivity of the index.</li>
                    <li>C1, C2 (Coefficients): Coefficients to correct for aerosol influences.</li>
                    <li>L (Canopy background adjustment): Canopy background adjustment to minimize soil background influences.</li>
                  </ul>
                </li>
                <li><strong>Normalized Difference Water Index (NDWI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NDWI} = \frac{(\text{Green} - \text{NIR})}{(\text{Green}+\text{NIR})}$$
                    </li>
                    <li><strong>Purpose: </strong>NDWI is used to detect the presence of water. Water absorbs more in the near-infrared region, resulting in a lower reflectance value.</li>
                    <li>Green: Reflectance in the green region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Soil Adjusted Vegetation Index (SAVI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{SAVI} = (1+L)\times \frac{(\text{NIR} - \text{Red})}{(\text{NIR}+\text{Red}+L)}$$
                    </li>
                    <li><strong>Purpose: </strong>SAVI is designed to reduce the influence of soil brightness on vegetation indices, making it useful in areas with varying soil conditions.</li>
                    <li>L (Soil adjustment factor): A soil adjustment factor to reduce the influence of soil brightness.</li>
                  </ul>
                </li>
                <li><strong>Normalized Burn Ratio (NBR):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NBR} = \frac{(\text{NIR} - \text{SWIR})}{(\text{NIR}+\text{SWIR})}$$
                    </li>
                    <li><strong>Purpose: </strong>NBR is used for post-fire assessment, as it highlights changes in vegetation cover and health after a fire.</li>
                    <li>SWIR (Shortwave Infrared): Reflectance in the shortwave infrared region of the electromagnetic spectrum.</li>
                  </ul>
                </li>
                <li><strong>Moisture Stress Index (MSI):</strong>
                  <ul>
                    <li><strong>Formula: </strong>
                      $$\text{NBR} = \frac{(\text{NIR} - \text{SWIR})}{(\text{NIR}+\text{SWIR})}$$
                    </li>
                    <li><strong>Purpose:</strong> MSI helps identify areas experiencing moisture stress by leveraging differences in water absorption in the near-infrared and shortwave infrared regions.</li>
                  </ul>
                </li>
              </ol>
              <p>These indices are calculated using the reflectance values from specific spectral bands, often captured by satellite or airborne sensors. 
              By applying these indices, remote sensing analysts can extract valuable information about the environment, monitor changes, and gain insights into various ecological and agricultural parameters.</p><br>

              <h3>Remote Sensing – Spectral Indices – Applications</h3>
              <p>Remote sensing and spectral indices have various applications, especially in the realm of data science and analytics. Spectral indices are mathematical calculations applied to remote sensing data to extract meaningful information. Some applications include:</p>
              <ol>
                <li>Vegetation Monitoring: Spectral indices like NDVI (Normalized Difference Vegetation Index) are widely used to assess vegetation health and monitor changes over time.</li>
                <li>Crop Health Assessment: Remote sensing helps in evaluating crop conditions, detecting diseases, and optimizing agricultural practices using indices such as NDVI or EVI (Enhanced Vegetation Index).</li>
                <li>Land Cover Classification: Spectral indices contribute to accurate land cover mapping, aiding in urban planning, environmental monitoring, and resource management.</li>
                <li>Water Quality Monitoring: Spectral indices can be employed to assess water quality in lakes, rivers, and oceans by detecting changes in water properties.</li>
                <li>Forest Management: Remote sensing assists in forest inventory, monitoring deforestation, and assessing overall forest health using indices tailored for vegetation analysis.</li>
                <li>Climate Change Studies: Spectral indices contribute to monitoring environmental changes, such as shifts in temperature, precipitation, and vegetation patterns, supporting climate change research.</li>
                <li>Disaster Management: Remote sensing helps in assessing and managing natural disasters like floods, wildfires, and earthquakes by providing real-time data for decision-making.</li>
              </ol>

<p>In the context of your interests in data science, analytics, and data engineering, integrating and analyzing remote sensing data using the mentioned tools and technologies can enhance insights and support decision-making in these domains.</p>


              <h3><span style="color:red"><strong>Radiance or Reflectance:</strong></span></h3>
              <p>Radiance and reflectance are two fundamental radiometric quantities used in remote sensing and satellite imagery to describe the properties of the reflected 
                or emitted electromagnetic radiation from surfaces on Earth. These quantities are used to quantify the amount of light or radiation observed by remote sensing 
                sensors. Here's an explanation of each, along with their formulas:</p>
              <ol>
                <li><strong>Radiance (L):</strong>
                  <ul>
                    <li><strong>Definition:</strong> Radiance measures the amount of electromagnetic radiation per unit 
                      area, per unit solid angle, and per unit wavelength interval. It describes the radiative energy 
                      received by a sensor from a particular direction and wavelength.</li>
                    <li><strong>Units:</strong> Radiance is typically expressed in watts per square meter per 
                      steradian per micrometer (W/(m²·sr·μm)).</li>
                    <li><strong>Formula:</strong> The formula for radiance is given by:
                    
                      \[L(\lambda) = \frac{\pi \cdot R(\lambda)}{E(\lambda)}\]
                      
                      <p>where:</p>
                        <ul>
                          <li>L(λ) is the radiance at wavelength λ.</li>
                          <li>R(λ) is the radiance received by the sensor from the target.</li>
                          <li>E(λ) is the effective spectral radiance of the sensor.</li>
                        </ul>
                    </li>
                  </ul><br>
                  <div style="background-color: #66a8a56b; padding: 15px; border-radius: 5px; border: 1px solid #000;">
                      <p>In the case of satellite measurments, the objective of ocean sensors is to retreive the spectral distribution of upwelling radiance 
                        just above the sea surface, which is termed the water leaving radiance (L<sub>w</sub>). However, the sensors actually measure the Top 
                        of Atmoshphere (TOA) radiance L<sub>t</sub> and so the contribution resulting from processes such as the atmosphere such as the 
                        atmoshphere's scattering and absorption needs to be accounted for -termed Atmospheric  Correction (AC).

                    $$L_t(\lambda) = L_r(\lambda)+ L_a(\lambda)+L_{ra}(\lambda)+t(\lambda)L_{wc}(\lambda)+T(\lambda)L_{g}(\lambda)+t(\lambda)t_0(\lambda)\text{cos}(\theta_0)
                    L_{wn}(\lambda)$$

                    where</p>
                    
                    <ul>
                    <li>L<sub>r</sub> due to Rayleigh scattering</li>
                    <li>L<sub>a</sub> due to aerosol scattering</li>
                    <li>L<sub>ra</sub> due to interaction aerosols and molecules</li>
                    <li>L<sub>wc</sub> due to interaction between white caps</li>
                    <li>L<sub>g</sub> due to  interaction between  glint.</li>
                    <li>t and t<sub>0</sub> are diffusive transmmitances of the atmohsphere from the surface to the senor and from the sun to the surface.</li>
                    <li>T is the direct transmittance from surface to sensor</li>
                    <li> θ<sub>0</sub> is the solar zenith angle </li>
                    <li>L<sub>wn</sub>(λ) is the normalized water leaving radiance.</li>
                    </ul>
                  </div><br>

                <li><strong>Reflectance (ρ) or Reflectance Factor (RF):</strong>
                  <ul>
                    <li><strong>Definition:</strong> Reflectance measures the ratio of reflected light from a surface to the incident light upon it. It quantifies how much of 
                      the incoming radiation is reflected by the surface. Reflectance is usually expressed as a dimensionless value between 0 and 1, but it can be multiplied 
                      by 100 to express it as a percentage.</li>
                    <li><strong>Units:</strong> Reflectance is a dimensionless quantity or percentage.</li>
                    <li><strong>Formula:</strong> The formula for reflectance is given by:
                    $$
                    \rho(\lambda) = \frac{L_{\rm reflected(\lambda)}}{L_{\rm incident(\lambda)}}
                    $$
                    where:
                    <ul>
                      <li>ρ(λ) is the reflectance at wavelength λ.</li>
                      <li>L<sub>reflected</sub>(λ) is the radiance of the reflected light from the target.</li>
                      <li>L<sub>incident</sub>(λ) is the radiance of the incident light on the target.</li>
                    </ul>
                  </ul>
              </ol>
              <p>The calculation of reflectance often involves radiometric calibration to convert sensor radiance measurements to physical units and account for atmospheric 
                effects. Reflectance is an important quantity in remote sensing because it allows for the comparison of data collected by different sensors or at different 
                times, making it a valuable tool for monitoring changes in land cover, vegetation health, and other Earth surface properties.</p>
              <p>Note that the specific calculation of reflectance can be more complex in practice, taking into account various factors like atmospheric correction, sensor 
                characteristics, and surface properties. The formula provided here is a simplified representation, and in practice, detailed algorithms and corrections may 
                be applied to obtain accurate reflectance values from remote sensing data.</p>
              <p>(Reference: <a href="https://training.eumetsat.int/mod/book/tool/print/index.php?id=11832">https://training.eumetsat.int/mod/book/tool/print/index.php?id=11832</a>)</p>
            </section>

              <h2>Calculation of physical parameters from the Radiance:</h2>

              <p>Once you have preprocessed raw data received from a remote sensing satellite and calculated radiance, you can use this radiance data to derive several 
                important physical parameters and information about the Earth's surface and atmosphere. The specific parameters you can calculate depend on the type of 
                remote sensing data, the spectral bands used, and the sensors' characteristics. Here are some common physical parameters that can be derived from radiance data:</p>

              <ol>
                  <li><strong>Surface Temperature:</strong>
                      <ul>
                          <li>Radiance data in thermal infrared bands can be used to calculate the surface temperature of the Earth's features. This is essential for 
                            applications like land surface temperature monitoring, agriculture, and urban heat island analysis.</li>
                      </ul>
                  </li>
                  
                  <li><strong>Vegetation Indices:</strong>
                      <ul>
                          <li>Radiance data in visible and near-infrared bands can be used to calculate vegetation indices such as the Normalized Difference Vegetation 
                            Index (NDVI) or Enhanced Vegetation Index (EVI). These indices provide information about vegetation health, density, and vigor.</li>
                      </ul>
                  </li>

                  <li><strong>Ocean Color Parameters:</strong>
                      <ul>
                          <li>Radiance data over water bodies can be used to calculate ocean color parameters such as chlorophyll-a concentration, water turbidity, and 
                            suspended particulate matter. These parameters are crucial for marine and coastal studies.</li>
                      </ul>
                  </li>

                  <li><strong>Aerosol Optical Depth (AOD):</strong>
                      <ul>
                          <li>Radiance data can be used to estimate the AOD, which quantifies the amount of aerosols (dust, smoke, pollution) in the atmosphere. AOD is 
                            vital for studying air quality, climate modeling, and visibility assessments.</li>
                      </ul>
                  </li>

                  <li><strong>Atmospheric Profiles:</strong>
                      <ul>
                          <li>Radiance data can be used in conjunction with radiative transfer models to derive vertical profiles of atmospheric parameters, including 
                            temperature, humidity, and aerosol content. These profiles are valuable for atmospheric research and weather forecasting.</li>
                      </ul>
                  </li>

                  <li><strong>Snow Cover and Albedo:</strong>
                      <ul>
                          <li>Radiance data can be used to estimate snow cover extent and calculate surface albedo, which measures the reflectivity of the Earth's surface. 
                            These parameters are essential for climate studies and snowmelt modeling.</li>
                      </ul>
                  </li>

                  <li><strong>Land Surface Emissivity:</strong>
                      <ul>
                          <li>For thermal infrared data, you can calculate land surface emissivity, which is essential for accurate surface temperature retrieval and energy 
                            balance studies.</li>
                      </ul>
                  </li>

                  <li><strong>Land Cover Classification:</strong>
                      <ul>
                          <li>Radiance data can be used for land cover classification and land use mapping, providing information on the types of land features present in a 
                            particular area.</li>
                      </ul>
                  </li>

                  <li><strong>Cloud Properties:</strong>
                      <ul>
                          <li>Radiance data can be used to retrieve cloud properties such as cloud height, cloud type, and cloud cover fraction. This information is vital 
                            for weather forecasting and climate studies.</li>
                      </ul>
                  </li>

                  <li><strong>Surface Roughness:</strong>
                      <ul>
                          <li>Radiance data can be used to estimate surface roughness, which is useful for applications like agriculture, hydrology, and soil moisture monitoring.</li>
                      </ul>
                  </li>
              </ol>

              <p>It's important to note that the process of deriving these physical parameters often involves complex radiative transfer models, atmospheric correction, and 
                calibration procedures. Additionally, the availability of specific bands and sensors on the remote sensing satellite can impact the types of parameters that 
                can be calculated. Therefore, the choice of satellite and sensor should align with the specific objectives of your remote sensing analysis.</p>

              <h3>Calculation of surface temperature:</h3>

                <p>Surface temperature is often calculated using radiance data acquired in the thermal infrared (TIR) spectral range. The process involves several steps, 
                  including atmospheric correction and radiative transfer modeling. Here's an overview of how surface temperature is calculated from radiance data:</p>

                <ol>
                    <li><strong>Radiance Measurement:</strong>
                        <ul>
                            <li>Radiance data is collected by remote sensing instruments operating in the TIR spectral range. These sensors capture the thermal radiation 
                              emitted by the Earth's surface in the form of radiance values.</li>
                        </ul>
                    </li>
                    
                    <li><strong>Atmospheric Correction:</strong>
                        <ul>
                            <li>Atmospheric correction is a critical step because the atmosphere absorbs and scatters thermal radiation. To calculate surface temperature 
                              accurately, you need to remove the atmospheric effects. This is typically done using radiative transfer models or atmospheric profile data.</li>
                        </ul>
                    </li>

                    <li><strong>Emissivity Correction:</strong>
                        <ul>
                            <li>Surface emissivity is a measure of how efficiently a surface emits thermal radiation. It can vary depending on the material and the wavelength 
                              of observation. To calculate surface temperature, you need to know or estimate the emissivity of the surface in the TIR band. Emissivity values 
                              are typically assigned based on land cover classes or measured in the field.</li>
                        </ul>
                    </li>

                    <li><strong>Planck's Law:</strong>
                        <ul>
                            <li>Planck's law describes the relationship between radiance, temperature, wavelength, and emissivity. It is used to relate the radiance measured 
                              by the sensor to the surface temperature and emissivity. The formula for Planck's law is:</li>
                        </ul>
                        <img src="https://latex.codecogs.com/svg.latex?L%28%5Clambda%2C%20T%29%20%3D%20%5Cfrac%7B2%20%5Cpi%20h%20c%5E2%7D%7B%5Clambda%5E5%7D%20%5Cfrac%7B1%7D%7Be%5E%7B%5Cfrac%7Bhc%7D%7B%5Clambda%20k%20T%7D%7D%20-1%7D">
                    </li>

                    <li><strong>Iterative Optimization:</strong>
                        <ul>
                            <li>Calculating surface temperature from radiance data involves an iterative optimization process. Given the measured radiance, known or estimated 
                              emissivity, and the Planck's law equation, an iterative optimization algorithm is used to find the temperature that best fits the observed radiance. 
                              This is typically done for each pixel in the image.</li>
                        </ul>
                    </li>

                    <li><strong>Output:</strong>
                        <ul>
                            <li>The result of this process is a surface temperature map, where each pixel represents the estimated temperature of the corresponding area on 
                              the Earth's surface.</li>
                        </ul>
                    </li>
                </ol>

                <p>It's important to note that accurate surface temperature retrieval can be influenced by various factors, including atmospheric conditions, sensor 
                  characteristics, and emissivity assumptions. Therefore, careful calibration, validation, and consideration of these factors are essential to ensure 
                  the reliability of the calculated surface temperatures in remote sensing applications.</p>


                <h4>Note:</h4>

                <ul>
                    <li><strong>Power/Radiation Flux:</strong> The rate at which energy is transferred per time t is known as power: 
                      $$P = \frac{dQ}{dt} ~~~~[J s^{-1}] = W$$.
                        <ul>
                            <li>When speaking about radiant power that is emitted by, passing through, or incident on a particular surface, the term flux ϕ is more commonly used.</li>
                        </ul>
                    </li>
                
                    <li><strong>Flux Density:</strong> The term flux density E refers to the spatial density of radiant power. It is defined as the infinitesimal amount 
                      of radiant power 
                      <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>&#x03C6;</mi><mo>(</mo><mi>&#x1D431;</mi><mo>,</mo><mi>&#x1D714;</mi><mo>)</mo></mrow></math>
                      passing through an infinitesimal surface element dA that is aligned normal to a direction θ
                      and located at a position <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math> of interest.
                        
                            $$dE(\vec{r}, \vec{\theta}) = \frac{d\phi(\vec{r}, \vec{\theta})}{dA}~~~ [W m^{-2}]$$

                    </li>
                
                    <li><strong>Radiance:</strong> The radiance, i.e., a measure of the radiant intensity that originates from a small unit area <d>dA</d><sup>&perp;</sup>
                      (aligned normal to the direction θ of interest) rather than from a single point:
                        
                        $$L(\vec{r}, \vec{\theta}) = \frac{d\phi^2(\vec{r}, \vec{\theta})}{dA^\perp d\Omega}$$
                        
                    </li>
                </ul>
                
                <img src="assets/img/portfolio/image-8.png" alt="Your Image" style="max-width: 500px;"/>
                
                <p>The quantity radiance is a quantity that is characteristic for a specific point <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math>
                   and a direction θ. It is defined as the amount of flux passing through a unit area <d>A</d><sup>&perp;</sup> (centered at <math xmlns="http://www.w3.org/1998/Math/MathML"><mover><mi>r</mi><mo>&rarr;</mo></mover></math> and aligned normal to θ) into a solid angle Ω around the direction of θ.</p>
                
                <p>(Reference: <a href="https://www.physics-in-a-nutshell.com/article/22/local-properties-of-radiation">https://www.physics-in-a-nutshell.com/article/22/local-properties-of-radiation</a>)</p>
                



              <h3>Reference:</h3>
                <ul style="margin-left: 30px;">
                  <li><a href="https://www.mdpi.com/2072-4292/12/16/2597" target="_blank">Evaluation Analysis of Landsat Level-1 and Level-2 Data Products Using In Situ Measurements, Cibele Teixeira Pinto, Xin Jing, Larry Leigh.</a></li>
                  <li><a href="https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing">Earthdata NASA - Remote Sensing</a></li>
                  <li><a href="https://en.wikipedia.org/wiki/Remote_sensing">Wikipedia - Remote Sensing</a></li>
                  <li><a href="https://giovanni.gsfc.nasa.gov/giovanni/#service=DiArAvTs&starttime=2022-01-01T00:00:00Z&endtime=2023-01-31T23:59:59Z&bbox=68.1152,5.8337,96.416,37.4744&data=OMAERUVd_003_FinalAerosolAbsOpticalDepth388%2COMAERUVd_003_FinalAerosolAbsOpticalDepth500&variableFacets=dataProductObservation%3AModel%2CObservation%3B">Giovanni NASA</a></li>
                  <li><a href="https://appliedsciences.nasa.gov/get-involved/training/english/arset-satellite-data-air-quality-environmental-justice-and-equity">NASA Applied Sciences - Satellite Data</a></li>
                  <li><a href="https://en.wikipedia.org/wiki/European_Organisation_for_the_Exploitation_of_Meteorological_Satellites">Wikipedia - European Organisation for the Exploitation of Meteorological Satellites</a></li>
                  <li><a href="https://www.atmospheremooc.org/">Atmosphere MOOC</a></li>
                  <li><a href="https://www.eumetsat.int/online-learning">EUMETSAT Online Learning</a></li>
                  <li><a href="https://www.eumetsat.int/data-and-user-support/training">EUMETSAT Data and User Support - Training</a></li>
                  <li><a href="https://login.ltpy.adamplatform.eu/authentication/login/?next=/">ADAM Platform Login</a></li>
                </ul>

                <div class="navigation">
                    <a href="index.html" class="clickable-box">
                        <span class="arrow-left">Go home</span>
                    </a>
                    <a href="Satellite-data.html" class="clickable-box">
                        <span class="arrow-right">Go to EU space mission</span>
                    </a>
                </div>
            </div>
        </div>
    </section><!-- End Portfolio Details Section -->

</main><!-- End #main -->

<!-- ======= Footer ======= -->
<footer id="footer">
  <div class="container">
    <div class="copyright">
      &copy; Copyright <strong><span>Arun</span></strong>
    </div>
    <div class="credits">
      <!-- All the links in the footer should remain intact. -->
      <!-- You can delete the links only if you purchased the pro version. -->
      <!-- Licensing information: https://bootstrapmade.com/license/ -->
      <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/ -->
      Designed by <a href="https://arunp77.github.io/Vision-Analytics/" target="_blank">Vision ANalytica</a>
    </div>
  </div>
</footer><!-- End  Footer -->

<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/typed.js/typed.umd.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    hljs.initHighlightingOnLoad();
  });
</script>

</body>

</html>